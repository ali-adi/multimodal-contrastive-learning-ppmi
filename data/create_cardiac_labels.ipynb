{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "BASE = \n",
    "TABULAR_BASE = join(BASE,'tabular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_array_equality(ob1, ob2):\n",
    "  if torch.is_tensor(ob1) or isinstance(ob1, np.ndarray):\n",
    "    assert (ob2 == ob1).all()\n",
    "  else:\n",
    "    assert ob2 == ob1\n",
    "\n",
    "def check_or_save(obj, path, index=None, header=None):\n",
    "  if isinstance(obj, pd.DataFrame):\n",
    "    if index is None or header is None:\n",
    "      raise ValueError('Index and header must be specified for saving a dataframe')\n",
    "    if os.path.exists(path):\n",
    "      if not header:\n",
    "        saved_df = pd.read_csv(path,header=None)\n",
    "      else:\n",
    "        saved_df = pd.read_csv(path)\n",
    "      naked_df = saved_df.reset_index(drop=True)\n",
    "      naked_df.columns = range(naked_df.shape[1])\n",
    "      naked_obj = obj.reset_index(drop=not index)\n",
    "      naked_obj.columns = range(naked_obj.shape[1])\n",
    "      if naked_df.round(6).equals(naked_obj.round(6)):\n",
    "        return\n",
    "      else:\n",
    "        diff = (naked_df.round(6) == naked_obj.round(6))\n",
    "        diff[naked_df.isnull()] = naked_df.isnull() & naked_obj.isnull()\n",
    "        assert diff.all().all(), \"Dataframe is not the same as saved dataframe\"\n",
    "    else:\n",
    "      obj.to_csv(path, index=index, header=header)\n",
    "  else:\n",
    "    if os.path.exists(path):\n",
    "      saved_obj = torch.load(path)\n",
    "      if isinstance(obj, list):\n",
    "        for i in range(len(obj)):\n",
    "          check_array_equality(obj[i], saved_obj[i])\n",
    "      else:\n",
    "        check_array_equality(obj, saved_obj)\n",
    "    else:\n",
    "      print(f'Saving to {path}')\n",
    "      torch.save(obj, path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label files for LVM (g) regression\n",
    "tabular_df = pd.read_csv(join(TABULAR_BASE,'cardiac_features_imputed_noOH.csv'))\n",
    "tabular_df.set_index('eid', inplace=True)\n",
    "\n",
    "train_ids = torch.load(join(TABULAR_BASE,'ids_train_tabular_imaging.pt'))\n",
    "val_ids = torch.load(join(TABULAR_BASE,'ids_val_tabular_imaging.pt'))\n",
    "test_ids = torch.load(join(TABULAR_BASE,'ids_test_tabular_imaging.pt'))\n",
    "\n",
    "def grab_target_in_split(df, _ids, target):\n",
    "    split_df = df.loc[_ids]\n",
    "    return list(split_df[target])\n",
    "\n",
    "targets = ['LVESV (mL)','LVEDV (mL)','LVSV (mL)','LVEF (%)','LVCO (L/min)','LVM (g)','RVEDV (mL)','RVESV (mL)','RVSV (mL)','RVEF (%)']\n",
    "for _ids, split in zip([train_ids, val_ids, test_ids], ['train', 'val', 'test']):\n",
    "    split_df = tabular_df.loc[_ids]\n",
    "    check_or_save(torch.tensor(split_df[targets].to_numpy(),dtype=torch.float32), join(TABULAR_BASE,f'labels_{split}_ImagingDerived_regression.pt'), header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infarct & CAD & Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICD Based Targets\n",
    "train_ids = torch.load(join(TABULAR_BASE,'ids_train_tabular_imaging.pt'))\n",
    "val_ids = torch.load(join(TABULAR_BASE,'ids_val_tabular_imaging.pt'))\n",
    "test_ids = torch.load(join(TABULAR_BASE,'ids_test_tabular_imaging.pt'))\n",
    "\n",
    "tabular_df = pd.read_csv(join(TABULAR_BASE,'cardiac_feature_668815_vector_labeled_noOH.csv'))\n",
    "tabular_df.set_index('eid', inplace=True)\n",
    "# Create multi-class label files for past and future ICD codes\n",
    "\n",
    "cleaned_features_path = join(TABULAR_BASE,'cardiac_features_668815_clean.csv')\n",
    "data_df = pd.read_csv(cleaned_features_path)\n",
    "\n",
    "date_attended_imaging = pd.read_csv(join(TABULAR_BASE,'col67.txt'))\n",
    "date_attended_imaging.rename(columns={'eid':'eid','53-2.0':'Date of attending imaging centre-2.0'},inplace=True)\n",
    "data_df_extended = data_df.merge(date_attended_imaging, left_on='eid', right_on='eid', how='inner')\n",
    "assert len(data_df_extended) == len(data_df)\n",
    "\n",
    "for target_name in ['CAD', 'Infarct', 'Diabetes']:\n",
    "  if target_name == 'CAD':\n",
    "    target = ['I200', 'I201', 'I208', 'I209', \n",
    "          'I220', 'I221', 'I228', 'I229',\n",
    "          'I210', 'I211', 'I212', 'I213', 'I214', 'I219',\n",
    "          'I240', 'I248', 'I249'\n",
    "          'I250', 'I251', 'I252', 'I253', 'I254', 'I255', 'I256', 'I258', 'I259']\n",
    "  elif target_name == 'Infarct':\n",
    "    target = ['I210', 'I211', 'I212', 'I213', 'I214', 'I219', 'I220', 'I221', 'I228', 'I229']\n",
    "  elif target_name == 'Diabetes':\n",
    "    target = ['E100','E101','E102','E103','E104','E105','E106','E107','E108','E109','E110','E111','E112','E113','E114','E115','E116','E117','E118','E119','E121','E123','E125','E128','E129','E130','E131','E132','E133','E134','E135','E136','E137','E138','E139','E140','E141','E142','E143','E144','E145','E146','E147','E148','E149']\n",
    "\n",
    "  array_length = 243\n",
    "  diag_name = 'Diagnoses - ICD10-0.'\n",
    "  date_name = 'Date of first in-patient diagnosis - ICD10-0.'\n",
    "  all_target_dates = []\n",
    "  all_target_indices = []\n",
    "  all_target_ids = []\n",
    "  for i in range(array_length):\n",
    "    all_target_dates.extend(list(data_df[data_df[f'{diag_name}{i}'].isin(target)][f'{date_name}{i}']))\n",
    "    all_target_indices.extend(list(data_df[data_df[f'{diag_name}{i}'].isin(target)].index))\n",
    "    all_target_ids.extend(list(data_df[data_df[f'{diag_name}{i}'].isin(target)]['eid']))\n",
    "\n",
    "  date_attending_centre = []\n",
    "  for i in all_target_indices:\n",
    "    date_attending_centre.append(data_df_extended.loc[i,'Date of attending imaging centre-2.0'])\n",
    "  date_attending_centre = pd.Series(date_attending_centre).astype('datetime64[ns]')\n",
    "\n",
    "  target_df = pd.DataFrame({'eid':all_target_ids,'target date':all_target_dates,'imaging date':date_attending_centre})\n",
    "  for time in ['all']:\n",
    "    if time == 'future':\n",
    "      target_ids = target_df[target_df['target date']>target_df['imaging date']]['eid']\n",
    "    elif time == 'past':\n",
    "      target_ids = target_df[target_df['target date']<target_df['imaging date']]['eid']\n",
    "    else:\n",
    "      target_ids = target_df['eid']\n",
    "\n",
    "    for _ids, balance, split in zip([train_ids, train_ids, val_ids, val_ids, test_ids], [True, False, True, False, False], ['train', 'train', 'val', 'val', 'test']):\n",
    "        addendum = ''\n",
    "        split_df = tabular_df.loc[_ids]\n",
    "        if balance:\n",
    "            addendum = '_balanced'\n",
    "            positive_df = split_df[split_df.index.isin(target_ids)]\n",
    "            negative_df = split_df[~split_df.index.isin(target_ids)]\n",
    "            negative_df_balanced = negative_df.sample(len(positive_df), random_state=2023)\n",
    "            subset_df = pd.concat([positive_df, negative_df_balanced])\n",
    "            subset_ids = list(subset_df.index)\n",
    "            subset_ids.sort()\n",
    "            check_or_save(subset_ids, join(TABULAR_BASE, f'ids_{split}_{target_name}_{time}_balanced.pt'))\n",
    "            split_df = tabular_df.loc[subset_ids]\n",
    "        labels = list(split_df.index.isin(target_ids).astype(int))\n",
    "        check_or_save(labels, join(TABULAR_BASE, f'labels_{split}_{target_name}_{time}{addendum}.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundukbb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
