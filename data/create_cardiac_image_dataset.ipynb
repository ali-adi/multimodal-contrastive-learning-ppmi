{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def check_array_equality(ob1, ob2):\n",
    "  if torch.is_tensor(ob1) or isinstance(ob1, np.ndarray):\n",
    "    assert (ob2 == ob1).all()\n",
    "  else:\n",
    "    assert ob2 == ob1\n",
    "\n",
    "def check_or_save(obj, path, index=None, header=None):\n",
    "  if isinstance(obj, pd.DataFrame):\n",
    "    if index is None or header is None:\n",
    "      raise ValueError('Index and header must be specified for saving a dataframe')\n",
    "    if os.path.exists(path):\n",
    "      if not header:\n",
    "        saved_df = pd.read_csv(path,header=None)\n",
    "      else:\n",
    "        saved_df = pd.read_csv(path)\n",
    "      naked_df = saved_df.reset_index(drop=True)\n",
    "      naked_df.columns = range(naked_df.shape[1])\n",
    "      naked_obj = obj.reset_index(drop=not index)\n",
    "      naked_obj.columns = range(naked_obj.shape[1])\n",
    "      if naked_df.round(6).equals(naked_obj.round(6)):\n",
    "        return\n",
    "      else:\n",
    "        diff = (naked_df.round(6) == naked_obj.round(6))\n",
    "        diff[naked_df.isnull()] = naked_df.isnull() & naked_obj.isnull()\n",
    "        assert diff.all().all(), \"Dataframe is not the same as saved dataframe\"\n",
    "    else:\n",
    "      obj.to_csv(path, index=index, header=header)\n",
    "  else:\n",
    "    if os.path.exists(path):\n",
    "      saved_obj = torch.load(path)\n",
    "      if isinstance(obj, list):\n",
    "        for i in range(len(obj)):\n",
    "          check_array_equality(obj[i], saved_obj[i])\n",
    "      else:\n",
    "        check_array_equality(obj, saved_obj)\n",
    "    else:\n",
    "      print(f'Saving to {path}')\n",
    "      torch.save(obj, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(tensor, gamma):\n",
    "    if tensor.min() < 0:\n",
    "        output = tensor.sign() * tensor.abs() ** gamma\n",
    "    else:\n",
    "        output = tensor ** gamma\n",
    "    return output\n",
    "\n",
    "class RandomGamma(torch.nn.Module):\n",
    "    def __call__(self, pic):\n",
    "        ran = np.random.uniform(low=0.25,high=1.75)\n",
    "        transformed_tensors = power(pic,ran)\n",
    "        return transformed_tensors\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(tensor, gamma):\n",
    "    if tensor.min() < 0:\n",
    "        output = tensor.sign() * tensor.abs() ** gamma\n",
    "    else:\n",
    "        output = tensor ** gamma\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, cx, cy, size):\n",
    "    \"\"\" Crop a 3D image using a bounding box centred at (cx, cy) with specified size. CHANNELS FIRST \"\"\"\n",
    "    X, Y = image.shape[1:]\n",
    "    r = int(size / 2)\n",
    "    x1, x2 = cx - r, cx + r\n",
    "    y1, y2 = cy - r, cy + r\n",
    "    x1_, x2_ = max(x1, 0), min(x2, X)\n",
    "    y1_, y2_ = max(y1, 0), min(y2, Y)\n",
    "    # Crop the image\n",
    "    crop = image[:, x1_: x2_, y1_: y2_]\n",
    "    # Pad the image if the specified size is larger than the input image size\n",
    "    if crop.ndim == 3:\n",
    "        crop = np.pad(crop,\n",
    "                      ((0, 0), (x1_ - x1, x2 - x2_), (y1_ - y1, y2 - y2_)),\n",
    "                      'constant')\n",
    "    elif crop.ndim == 4:\n",
    "        crop = np.pad(crop,\n",
    "                      ((0, 0), (0, 0), (x1_ - x1, x2 - x2_), (y1_ - y1, y2 - y2_)),\n",
    "                      'constant')\n",
    "    else:\n",
    "        print('Error: unsupported dimension, crop.ndim = {0}.'.format(crop.ndim))\n",
    "        exit(0)\n",
    "    return crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to parse the original nii images into stacks for training, run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert path to folder containing all downloaded subject folders here\n",
    "image_base_folder = \n",
    "\n",
    "def get_mid_beat_slice(im, es_slice):\n",
    "    thresh=(1.0, 99.0)\n",
    "    best_overlap_es = 0\n",
    "    for i in range(50):\n",
    "        im_slice = im[:,:,im.shape[2]//2,i]\n",
    "        overlap_es = (es_slice==im_slice).sum()\n",
    "        if overlap_es > best_overlap_es:\n",
    "            best_overlap_es = overlap_es\n",
    "            best_i_es = i\n",
    "\n",
    "    val_l, val_h = np.percentile(im, thresh)\n",
    "    im_slice = im[:,:,im.shape[2]//2,best_i_es]\n",
    "    im_slice[im_slice > val_h] = val_h\n",
    "    try:\n",
    "        assert np.allclose(im_slice,es_slice)\n",
    "    except:\n",
    "        return None\n",
    "    mid_beat_i = best_i_es//2\n",
    "    mid_beat_slice = im[:,:,im.shape[2]//2,mid_beat_i]\n",
    "    mid_beat_slice[mid_beat_slice > val_h] = val_h\n",
    "    return mid_beat_slice\n",
    "\n",
    "all_subjects = {}\n",
    "\n",
    "problem_ids = []\n",
    "missing_ids = []\n",
    "\n",
    "for folder in glob.glob(image_base_folder):\n",
    "    _id = folder.split('/')[-1]\n",
    "\n",
    "    if _id in all_subjects:\n",
    "        continue\n",
    "        \n",
    "    to_stack = []\n",
    "    es_slice = None\n",
    "    for cycle_position in ['sa_ES.nii.gz', 'sa.nii.gz', 'sa_ED.nii.gz']:\n",
    "        path = join(folder,cycle_position)\n",
    "        if os.path.exists(path):\n",
    "            nii = nib.load(path)\n",
    "            im = nii.get_fdata()\n",
    "            \n",
    "            # Too few z-axis slices are bad quality images\n",
    "            if im.shape[2] <= 7:\n",
    "                print(f'Too few z-axis slices: {folder}')\n",
    "                break\n",
    "                \n",
    "            # Full cycle volumes are used to extract middle of heart beat slice\n",
    "            if cycle_position == 'sa.nii.gz':\n",
    "                mid_heart_slice = get_mid_beat_slice(im, es_slice)\n",
    "                if mid_heart_slice is None:\n",
    "                    print(f'ES didnt match: {folder}')\n",
    "                    break\n",
    "            else:\n",
    "                mid_heart_slice = im[:,:,im.shape[2]//2]\n",
    "            \n",
    "            # Set es_slice to be used during extraction of mid beat\n",
    "            if cycle_position == 'sa_ES.nii.gz':\n",
    "                es_slice = mid_heart_slice\n",
    "                \n",
    "            # Pad to be square.\n",
    "            if mid_heart_slice.shape[1]>mid_heart_slice.shape[0]:\n",
    "                mid_heart_slice = np.pad(mid_heart_slice, ((((mid_heart_slice.shape[1]-mid_heart_slice.shape[0])//2), ((mid_heart_slice.shape[1]-mid_heart_slice.shape[0])//2)), (0, 0)), 'constant', constant_values=0)\n",
    "            else:\n",
    "                mid_heart_slice = np.pad(mid_heart_slice, ((0, 0), (((mid_heart_slice.shape[0]-mid_heart_slice.shape[1])//2), ((mid_heart_slice.shape[0]-mid_heart_slice.shape[1])//2))), 'constant', constant_values=0)\n",
    "            try:\n",
    "                assert mid_heart_slice.shape[0]==mid_heart_slice.shape[1], print(mid_heart_slice.shape[0], mid_heart_slice.shape[1])\n",
    "            except:\n",
    "                print(f'Shapes didnt match: {folder}')\n",
    "                break\n",
    "    \n",
    "            im_t = torch.tensor(mid_heart_slice)\n",
    "            to_stack.append(im_t)\n",
    "        else:\n",
    "            missing_ids.append(_id)\n",
    "            print(f'Missing files: {folder}')\n",
    "            break\n",
    "    if len(to_stack)==3:\n",
    "        ims_stacked_t_n = torch.stack(to_stack)\n",
    "        if ims_stacked_t_n.shape==(3,208,208):\n",
    "            ims_stacked_t_n=np.pad(ims_stacked_t_n, ((0,0),(1,1),(1,1)), 'constant', constant_values=0)\n",
    "        all_subjects[_id] = ims_stacked_t_n\n",
    "    else:\n",
    "        problem_ids.append(folder)\n",
    "\n",
    "torch.save(all_subjects, 'preprocessed_cardiac_dict.pt')\n",
    "torch.save(problem_ids, 'problem_ids_cardiac.pt')\n",
    "torch.save(missing_ids, 'missing_ids_cardiac.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the 208x208 to 210x210\n",
    "all_shapes = []\n",
    "for key, im in all_subjects.items():\n",
    "    if im.shape==(3,208,208):\n",
    "        im=np.pad(im, ((0,0),(1,1),(1,1)), 'constant', constant_values=0)\n",
    "        all_subjects[key]=im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify only 210x210 left\n",
    "all_shapes = []\n",
    "for i in all_subjects.values():\n",
    "    all_shapes.append(tuple(i.shape))\n",
    "set(all_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_subjects, 'preprocessed_cardiac_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random quality control\n",
    "random.seed(2025)\n",
    "keys = list(all_subjects.keys())\n",
    "f, axarr = plt.subplots(10, 3, figsize=(20,50))\n",
    "for i in range(10):\n",
    "    rand_idx = random.randrange(0,len(all_subjects))\n",
    "    im = all_subjects[keys[rand_idx]]\n",
    "    axarr[i,0].imshow(im[0,:,:])\n",
    "    axarr[i,1].imshow(im[1,:,:])\n",
    "    axarr[i,2].imshow(im[2,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \n",
    "TABULAR_BASE = join(BASE,'tabular')\n",
    "\n",
    "all_subjects = torch.load('/home/paulhager/Projects/data/cardiac/668815/preprocessed_cardiac_dict.pt')\n",
    "len(all_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random quality control\n",
    "random.seed(2030)\n",
    "keys = list(all_subjects.keys())\n",
    "f, axarr = plt.subplots(10, 3, figsize=(20,50))\n",
    "for i in range(10):\n",
    "    rand_idx = random.randrange(0,len(all_subjects))\n",
    "    im = all_subjects[keys[rand_idx]]\n",
    "    assert im.shape == (3,210,210)\n",
    "    axarr[i,0].imshow(im[0,:,:])\n",
    "    axarr[i,1].imshow(im[1,:,:])\n",
    "    axarr[i,2].imshow(im[2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_df = pd.read_csv(join(TABULAR_BASE,'cardiac_feature_668815_vector_labeled_noOH.csv'))\n",
    "tabular_df.set_index('eid', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_ids = list(tabular_df.index)\n",
    "\n",
    "print(f'There are {len(all_subjects)} images in the dataset')\n",
    "print(f'There are {len(tabular_ids)} tabular entries in the dataset')\n",
    "\n",
    "imaging_ids = list(all_subjects.keys())\n",
    "imaging_ids = [int(i) for i in imaging_ids]\n",
    "imaging_ids.sort()\n",
    "\n",
    "overlap_ids = [i for i in imaging_ids if i in tabular_ids]\n",
    "\n",
    "print(f'There are {len(overlap_ids)} overlap between images and tabular entries in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, rest_ids = train_test_split(overlap_ids, test_size=0.3, random_state=2023)\n",
    "val_ids, test_ids = train_test_split(rest_ids, test_size=0.5, random_state=2023)\n",
    "\n",
    "check_or_save(train_ids, join(TABULAR_BASE,'ids_train_tabular_imaging.pt'))\n",
    "check_or_save(val_ids, join(TABULAR_BASE,'ids_val_tabular_imaging.pt'))\n",
    "check_or_save(test_ids, join(TABULAR_BASE,'ids_test_tabular_imaging.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundukbb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
